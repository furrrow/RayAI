{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started - Distributed Model Training #\n",
    "Ray's quickstart on Distributed Model Training:\n",
    "https://docs.ray.io/en/latest/ray-overview/getting-started.html\n",
    "\n",
    "combined with Torch's Quickstart guide:\n",
    "https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "# device = (\n",
    "#     \"cuda\"\n",
    "#     if torch.cuda.is_available()\n",
    "#     else \"mps\"\n",
    "#     if torch.backends.mps.is_available()\n",
    "#     else \"cpu\"\n",
    "# )\n",
    "device=\"cpu\"\n",
    "\n",
    "class NaiveDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.flatten(inputs)\n",
    "        logits = self.linear_relu_stack(inputs)\n",
    "        return logits\n",
    "\n",
    "def train_func():\n",
    "    num_epochs=2\n",
    "    batch_size=128\n",
    "    model_state_name = \"serial_model.pth\"\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "\n",
    "    model = NaiveDense().to(device)\n",
    "    print(f\"Using {device} device\")\n",
    "    print(model)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    size = len(train_dataloader.dataset)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"training epoch [{epoch}/{num_epochs}]\")\n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), (batch + 1) * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    torch.save(model.state_dict(), model_state_name)\n",
    "    print(f\"Saved PyTorch Model State to {model_state_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training function can now be executed. note the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# good to run this anyway to download the packages to tmp if first time\n",
    "train_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(model_state=\"model.pth\"):\n",
    "    # note that the dataloader handles the last batch not matching\n",
    "    # the batch size by automatically adjusting the batch size\n",
    "    batch_size=512\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "    model = NaiveDense().to(device)\n",
    "    model.load_state_dict(torch.load(model_state))\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    size = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    for epoch in range(1):\n",
    "        # test_total = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = model(X)\n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                # test_total += len(y)\n",
    "                # print(f\"batch size: {len(y)}, Test total: {test_total} / {size}\")\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T07:33:30.747653818Z",
     "start_time": "2023-05-14T07:33:30.050160644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.797402 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_func(\"serial_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now letâ€™s convert this to a distributed multi-worker training function!\n",
    "\n",
    "All you have to do is use the **ray.train.torch.prepare_model** and **ray.train.torch.prepare_data_loader** utility functions to easily setup your model & data for distributed training. This will automatically wrap your model with DistributedDataParallel and place it on the right device, and add DistributedSampler to your DataLoaders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T07:17:36.573079002Z",
     "start_time": "2023-05-14T07:17:36.490629798Z"
    }
   },
   "outputs": [],
   "source": [
    "import ray.train.torch\n",
    "from ray import train\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air import session, Checkpoint, RunConfig, CheckpointConfig, ScalingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T07:22:20.102693040Z",
     "start_time": "2023-05-14T07:22:20.027098961Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_func_distributed(config):\n",
    "    batch_size=128\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "    train_dataloader = train.torch.prepare_data_loader(train_dataloader)\n",
    "\n",
    "    model = NaiveDense()\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    size = len(train_dataloader.dataset)\n",
    "    acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "    # for averaging loss\n",
    "    mean_valid_loss = torchmetrics.MeanMetric()\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            # Compute prediction error\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # evaluate\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = model(X)\n",
    "                valid_loss = loss_fn(pred, y)\n",
    "                # save loss in aggregator\n",
    "                mean_valid_loss(valid_loss)\n",
    "                acc(pred, y)\n",
    "\n",
    "            # collect all metrics\n",
    "            # use .item() to obtain a value that can be reported\n",
    "            valid_loss = valid_loss.item()\n",
    "            accuracy_collected = acc.compute().item()\n",
    "            mean_valid_loss_collected = mean_valid_loss.compute().item()\n",
    "\n",
    "            # save checkpoints\n",
    "            state_dict = model.state_dict()\n",
    "            checkpoint = Checkpoint.from_dict(\n",
    "                dict(epoch=epoch, model_weights=state_dict)\n",
    "            )\n",
    "            session.report({\n",
    "                \"accuracy_collected\": accuracy_collected,\n",
    "                \"valid_loss\": valid_loss,\n",
    "                \"mean_valid_loss_collected\": mean_valid_loss_collected,\n",
    "            }, checkpoint=checkpoint)\n",
    "\n",
    "            # reset for next epoch\n",
    "            acc.reset()\n",
    "            mean_valid_loss.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note setting GPU=True here does not work unless you actually have multiple GPUs!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T07:34:59.889904036Z",
     "start_time": "2023-05-14T07:34:04.805555978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 03:34:04,861\tINFO data_parallel_trainer.py:357 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "\u001b[2m\u001b[36m(TrainTrainable pid=141220)\u001b[0m 2023-05-14 03:34:06,621\tINFO data_parallel_trainer.py:357 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=141220)\u001b[0m 2023-05-14 03:34:06,624\tINFO data_parallel_trainer.py:357 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=141265)\u001b[0m 2023-05-14 03:34:12,335\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=5]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=141265)\u001b[0m 2023-05-14 03:34:13,908\tINFO train_loop_utils.py:286 -- Moving model to device: cpu\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=141265)\u001b[0m 2023-05-14 03:34:13,909\tINFO train_loop_utils.py:346 -- Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  accuracy_collected</th><th>date               </th><th>done  </th><th>hostname   </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">  mean_valid_loss_collected</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  valid_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_b429c_00000</td><td style=\"text-align: right;\">           0.0984375</td><td>2023-05-14_03-34-14</td><td>False </td><td>jim-desktop</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">                     2.2977</td><td>192.168.0.135</td><td style=\"text-align: right;\">141220</td><td>True               </td><td style=\"text-align: right;\">             7.49983</td><td style=\"text-align: right;\">           7.49983</td><td style=\"text-align: right;\">       7.49983</td><td style=\"text-align: right;\"> 1684049654</td><td style=\"text-align: right;\">                   1</td><td>b429c_00000</td><td style=\"text-align: right;\">     2.29408</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 03:34:59,881\tINFO tune.py:945 -- Total run time: 55.03 seconds (55.02 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_collected': 0.6187499761581421, 'valid_loss': 1.4619611501693726, 'mean_valid_loss_collected': 1.491119623184204, 'timestamp': 1684049697, 'time_this_iter_s': 0.11327934265136719, 'should_checkpoint': True, 'done': True, 'training_iteration': 282, 'trial_id': 'b429c_00000', 'date': '2023-05-14_03-34-57', 'time_total_s': 50.72310280799866, 'pid': 141220, 'hostname': 'jim-desktop', 'node_ip': '192.168.0.135', 'config': {'train_loop_config': {'num_epochs': 3}}, 'time_since_restore': 50.72310280799866, 'iterations_since_restore': 282, 'experiment_tag': '0'}\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# For GPU Training, set `use_gpu` to True.\n",
    "use_gpu = False\n",
    "checkpoint_config = CheckpointConfig(\n",
    "    num_to_keep=1, checkpoint_score_attribute=\"valid_loss\", checkpoint_score_order=\"min\"\n",
    ")\n",
    "trainer = TorchTrainer(\n",
    "    train_func_distributed,\n",
    "    train_loop_config={\"num_epochs\": 3},\n",
    "    scaling_config=ScalingConfig(num_workers=5, use_gpu=use_gpu),\n",
    "    run_config=RunConfig(checkpoint_config=checkpoint_config)\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T07:34:59.892872104Z",
     "start_time": "2023-05-14T07:34:59.888718692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(TorchCheckpoint(local_path=/home/jim/ray_results/TorchTrainer_2023-05-14_03-34-04/TorchTrainer_b429c_00000_0_2023-05-14_03-34-04/checkpoint_000281),\n",
       "  {'accuracy_collected': 0.6187499761581421,\n",
       "   'valid_loss': 1.4619611501693726,\n",
       "   'mean_valid_loss_collected': 1.491119623184204,\n",
       "   'timestamp': 1684049697,\n",
       "   'time_this_iter_s': 0.11327934265136719,\n",
       "   'should_checkpoint': True,\n",
       "   'done': False,\n",
       "   'training_iteration': 282,\n",
       "   'trial_id': 'b429c_00000',\n",
       "   'date': '2023-05-14_03-34-57',\n",
       "   'time_total_s': 50.72310280799866,\n",
       "   'pid': 141220,\n",
       "   'hostname': 'jim-desktop',\n",
       "   'node_ip': '192.168.0.135',\n",
       "   'time_since_restore': 50.72310280799866,\n",
       "   'iterations_since_restore': 282,\n",
       "   'experiment_tag': '0',\n",
       "   'config/train_loop_config/num_epochs': 3})]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.checkpoint  # last saved checkpoint\n",
    "result.best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T07:34:59.897770126Z",
     "start_time": "2023-05-14T07:34:59.891078960Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dict = result.best_checkpoints[0][0].to_dict()\n",
    "torch.save(checkpoint_dict.get(\"model_weights\"), \"parallel_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T07:35:00.564352892Z",
     "start_time": "2023-05-14T07:34:59.898895225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.508966 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_func(\"parallel_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unsure why the distributed version has a much lower accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
