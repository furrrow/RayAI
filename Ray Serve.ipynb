{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Testing out Ray's ML Serving functionalities (model as a service) #\n",
    "Getting started on ray-serve:\n",
    "https://docs.ray.io/en/latest/serve/getting_started.html#composing-machine-learning-models-with-deployment-graphs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# File name: summary_model.py\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from starlette.requests import Request\n",
    "import ray\n",
    "from ray import serve"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T19:51:02.928524702Z",
     "start_time": "2023-05-16T19:51:01.802330609Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it was the best of times, it was worst of times .\n"
     ]
    }
   ],
   "source": [
    "class LocalSummarizer:\n",
    "    def __init__(self):\n",
    "        # Load model\n",
    "        self.model = pipeline(\"summarization\", model=\"t5-small\")\n",
    "\n",
    "    def summarize(self, text: str) -> str:\n",
    "        # Run inference\n",
    "        model_output = self.model(text, min_length=5, max_length=15)\n",
    "\n",
    "        # Post-process output to return only the summary text\n",
    "        summary = model_output[0][\"summary_text\"]\n",
    "\n",
    "        return summary\n",
    "\n",
    "\n",
    "local_summarizer = LocalSummarizer()\n",
    "\n",
    "summary = local_summarizer.summarize(\n",
    "    \"It was the best of times, it was the worst of times, it was the age \"\n",
    "    \"of wisdom, it was the age of foolishness, it was the epoch of belief\"\n",
    ")\n",
    "print(summary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T19:51:03.776530527Z",
     "start_time": "2023-05-16T19:51:02.926215925Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### an example of ray deployment graph, see graph.py\n",
    "next run the following command in terminal to start the service\n",
    "edit: I modified and created a graph with just the summarization.\n",
    "- serve run translation_graph:deployment_graph\n",
    "- serve run summary_graph:deployment_graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'it was the best of times, it was worst of times .'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File name: graph_client.py\n",
    "import requests\n",
    "\n",
    "english_text = (\n",
    "    \"It was the best of times, it was the worst of times, it was the age \"\n",
    "    \"of wisdom, it was the age of foolishness, it was the epoch of belief\"\n",
    ")\n",
    "response = requests.post(\"http://127.0.0.1:8000/\", json=english_text)\n",
    "response.text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T20:02:15.397412176Z",
     "start_time": "2023-05-16T20:02:15.097594388Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'the primary interface to PyTorch is the Python programming language '"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text = \"As its name suggests, the primary interface to PyTorch is the Python programming language. While Python is a suitable and preferred language for many scenarios requiring dynamism and ease of iteration, there are equally many situations where precisely these properties of Python are unfavorable. One environment in which the latter often applies is production â€“ the land of low latencies and strict deployment requirements. For production scenarios, C++ is very often the language of choice, even if only to bind it into another language like Java, Rust or Go. The following paragraphs will outline the path PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python.\"\n",
    "response = requests.post(\"http://127.0.0.1:8000/\", json=my_text)\n",
    "response.text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T20:03:09.048923446Z",
     "start_time": "2023-05-16T20:03:08.659604259Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
